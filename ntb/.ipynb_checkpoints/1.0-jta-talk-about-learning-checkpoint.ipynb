{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Talking About Learning\n",
    "\n",
    "### What Do We Mean When We Talk About Learning?\n",
    "\n",
    "#### João Tiago Ascensão, Data Science @ Feedzai (soon) & LDSA, Ex-Farfetch, Ex-Uniplaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Table of contents\n",
    "\n",
    "1. Problem Statement (5 mins)\n",
    "2. Components of a Learning Algorithm (10 mins)\n",
    "    1. Loss Function\n",
    "    2. Cost Function\n",
    "    3. Optimization Routine\n",
    "3. Generalization (5 mins)\n",
    "4. Real Life (10 mins)\n",
    "5. Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Problem Statement\n",
    "\n",
    "$D = \\{(x_i, y_i)\\}_{i=1}^m$ is the *data*, a collection of labeled observations (or *instances*).\n",
    "\n",
    "$x_i = (x_i^{(1)},\\dots, x_i^{(n)}) \\in \\mathbb{R}^m$ is a *representation*, as a vector of *features*. $x_i^j$ is the *measurement* of the $j$-th feature for $i$-th observation $i$. Instances $x_i \\sim \\mathcal{D}$.\n",
    "\n",
    "The *target* $y_i$ results from a constant, unknown function $y_i = c(x_i), c \\in C$.\n",
    "\n",
    "We aim to infer $c$, the function that maps inputs $x_i$ to outputs $y_i$, from the data:\n",
    "\n",
    "$$\\hat{y}_i = h(x_i) \\approx c(x_i), h \\in H, H \\subseteq C$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Problem Statement\n",
    "\n",
    "$X \\in \\mathbb{R}^{m{\\times}n}$ contains the observations in our dataset.\n",
    "\n",
    "$$ X =\n",
    "\\begin{bmatrix}\n",
    "    x_{1}^{(1)}  & x_{1}^{(2)} & x_{1}^{(3)} & \\dots & x_{1}^{(n)} \\\\\n",
    "    x_{2}^{(1)}  & x_{2}^{(2)} & x_{2}^{(3)} & \\dots & x_{2}^{(n)} \\\\\n",
    "    \\dots        & \\dots       & \\dots       & \\dots & \\dots       \\\\\n",
    "    x_{m}^{(1)}  & x_{m}^{(2)} & x_{m}^{(3)} & \\dots & x_{m}^{(n)}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Problem Statement\n",
    "\n",
    "$Y \\in \\mathbb{R}^{m{\\times}1}$ contains the observed values for the target, for each observation.\n",
    "\n",
    "$$ Y =\n",
    "\\begin{bmatrix}\n",
    "    y_{1} \\\\\n",
    "    y_{2} \\\\\n",
    "    \\dots \\\\\n",
    "    y_{m}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![traditional-programming-vs-machine-learning](static/assets/traditional-programming-vs-machine-learning.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Components of a Learning Algorithm\n",
    "\n",
    "A learning algoritms aims to select the best available hypothesis $h \\in H$.\n",
    "\n",
    "The final hypothesis, more precisely, $h_\\Theta(x)$, is a combination of:\n",
    "* A general hypothesis (i.e., $H$), embedded in the learning algorithm \n",
    "* A finite set of parameter values, $\\Theta$, that adapt the general hypothesis to the data.\n",
    "\n",
    "Take linear regression: $h_\\Theta(x) = \\theta_0 + \\theta_1x_1 + \\dots + \\theta_nx_n$.\n",
    "\n",
    "It follows that if $H \\subseteq C$, **there's no guarantee that $c \\in H$**. *No magic applies.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![traditional-programming-vs-machine-learning-with-hypothesis](static/assets/traditional-programming-vs-machine-learning-with-hypothesis.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Components of a Learning Algorithm\n",
    "\n",
    "An hypothesis $h$ is said to be *consistent* with the data if it *fits* the data.\n",
    "\n",
    "The **loss function**, or error, is the a penalty for failing to predict a single example $i$.\n",
    "\n",
    "An example is the squared error loss: $L(h_\\Theta(x_i), y_i) = (h_\\Theta(x) - y_i)^2$.\n",
    "\n",
    "In theory, we can evaluate all possibilities $h_\\Theta \\in H$ against any $(x_i, y_i) \\in D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Components of a Learning Algorithm\n",
    "\n",
    "The **cost function** computes the error over the entire data set. \n",
    "\n",
    "Take the average loss, $J(\\Theta) = \\frac{1}{m}\\sum_{i=1}^{m} L(h_\\Theta(x_i), y_i)$, for example.\n",
    "\n",
    "Thus, we can define an optimization criterion for a learning algorithm: $\\min_\\Theta J(\\Theta)$.\n",
    "\n",
    "An hypothesis is perfectly consistent if $J(\\Theta) = 0$. (Not necessarily a good thing.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Components of a Learning Algorithm\n",
    "\n",
    "Algorithms that can approximate any target concept are *universal approximators*. \n",
    "\n",
    "This is different from *completeness*, i.e., searching the entire space of $C$, or $H$.\n",
    "\n",
    "(Hence, optimization. Guessing and/or random search won't take us far.)\n",
    "\n",
    "Closed-form is preferable to complex optimization, but often not possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Components of a Learning Algorithm\n",
    "\n",
    "Gradient descent minimizes an objective function $J(\\Theta)$, parameterized by $\\Theta$.\n",
    "\n",
    "It works by updating the parameters in the opposite direction of the gradient of the objective function $\\Delta_\\theta J(\\Theta)$, with respect to the parameters.\n",
    "\n",
    "The vanilla gradient descent (i.e., batch) runs in *epochs*, each epoch using *all data* to update each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Components of a Learning Algorithm\n",
    "\n",
    "The learning rate, $\\alpha$, determine the size of the steps we take to reach a minimum: \n",
    "\n",
    "$$\\theta_{t + 1} = \\theta_t - \\alpha \\Delta_{\\theta} J(\\Theta)$$\n",
    "\n",
    "The $\\Delta_{\\theta} J(\\Theta)$ concerns to the partial derivative of $J(\\Theta)$, with respect to $\\theta$. \n",
    "\n",
    "Intuitively, we follow *downhill* on the surface of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Generalization\n",
    "\n",
    "Given all we've seen, having a learning algorithm and some data doesn't ensure success:\n",
    "* The concept $c \\in C$ must be approximable by our learning algorithm\n",
    "* If we use the data set $D$ to fine-tune our general hypothesis, the sample must be representative and contain good, non-sparse, consistently measured features.\n",
    "\n",
    "And although both conditions are necessary, they are not sufficient for *generalization*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generalization\n",
    "\n",
    "Generalization is the ability of the trained hypothesis $h_\\Theta(x)$ to make good predictions on unseen data, assuming that incoming instances $x \\sim \\mathcal{D}$.\n",
    "\n",
    "It requires balancing in-sample and out-of-sample consistency, in what is known as the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generalization\n",
    "\n",
    "Simplistic algorithms with limited sets of hypothesis $H$ can't quite stretch and adapt to underlying patterns in the data, also known as *under-fitting*.\n",
    "\n",
    "Highly-flexible learning algorithms are great at consistency but prone to *overfitting*, i.e., reacting to noise and corner-cases in the data with increasing complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Generalization\n",
    "\n",
    "In both cases, the learning algorithm will select a bad hypothesis: wildly inconsistent or consistently wrong.\n",
    "\n",
    "Therefore, **the trade-off between bias (i.e., stupid) and variance (i.e., overthinking)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# No such thing as a free-lunch\n",
    "\n",
    "The no free lunch theorem proves that all learning algorithms are equivalent, when their performance is averaged across all possible problems.\n",
    "\n",
    "So, on average, random search would be as good as fancy optimization.\n",
    "\n",
    "It follows that some algorithms will perform better than other on specific problems, due to encoded prior knowledge, fit to the problem space at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Real Life\n",
    "\n",
    "![learning-loop](static/assets/learning-loop.svg)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
